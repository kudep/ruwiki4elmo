{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import sys\n",
    "import re\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_of_dirs = 'results'\n",
    "results_dir = 'outputs/results.step2' \n",
    "tokens_file = 'tokens_set.txt'\n",
    "chars_file = 'chars_set.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_file = os.path.join(results_dir, tokens_file)\n",
    "chars_file = os.path.join(results_dir, chars_file)\n",
    "os.makedirs(results_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counters_merge(counters):\n",
    "    if len(counters) < 3:\n",
    "        share_counter = collections.Counter()\n",
    "        for counter in counters:\n",
    "            share_counter += counter\n",
    "        return share_counter\n",
    "    else:\n",
    "        split_point = len(counters)//2\n",
    "        l_c = counters_merge(counters[:split_point])\n",
    "        r_c = counters_merge(counters[split_point:])\n",
    "        return l_c + r_c\n",
    "    \n",
    "def count_chars(text_lines):\n",
    "    counters = []\n",
    "    for text in text_lines:\n",
    "        counters.append(collections.Counter(text))\n",
    "    return counters_merge(counters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chars_counter_from_files(txt_files):\n",
    "    texts = [open(file).read().split('\\n') for file in txt_files]\n",
    "    lines = []\n",
    "    for txt_lines in texts:\n",
    "        lines.extend([''.join(line.split()) for line in txt_lines if line!='<NEXT_PAPER>' and line])\n",
    "\n",
    "    return count_chars(lines)\n",
    "\n",
    "def map_text_files(from_txt_files, to_txt_files, mapper):\n",
    "    texts = [open(file).read().split('\\n') for file in from_txt_files]\n",
    "    mapped_texts = []\n",
    "    for txt_lines in texts:\n",
    "        mapped_texts.append([ ''.join(map(mapper,line)) for line in txt_lines if line!='<NEXT_PAPER>' and line])\n",
    "    \n",
    "    for lines, file in zip(mapped_texts,to_txt_files):\n",
    "        open(file, 'wt').write('\\n'.join(lines)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [16:07<00:00,  3.75s/it]\n"
     ]
    }
   ],
   "source": [
    "dirs = [os.path.join(dir_of_dirs,file_path) for file_path in  os.listdir(dir_of_dirs)]\n",
    "counters = []\n",
    "for data_dir in tqdm.tqdm(dirs):\n",
    "    txt_files = [os.path.join(data_dir,file_path) for file_path in  os.listdir(data_dir)]\n",
    "    counters.append(get_chars_counter_from_files(txt_files))\n",
    "sum_counter = sum(counters, collections.Counter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = [char for char, c in sum_counter.most_common(255)] + ['ðŸ˜Ÿ'] #U+1F61F\n",
    "freq_chars = [' '] + chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = lambda char: char if char in freq_chars else freq_chars[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 258/258 [11:33<00:00,  2.69s/it]\n"
     ]
    }
   ],
   "source": [
    "dirs = [os.path.join(dir_of_dirs,file_path) for file_path in  os.listdir(dir_of_dirs)]\n",
    "file_index = 1\n",
    "for data_dir in tqdm.tqdm(dirs):\n",
    "    from_txt_files = [os.path.join(data_dir,file_path) for file_path in  os.listdir(data_dir)]\n",
    "    to_txt_files = [os.path.join(results_dir,'prts_{}'.format(str(_file_index).zfill(12))) for _file_index, file_path in  enumerate(os.listdir(data_dir),file_index)]\n",
    "    file_index += len(to_txt_files)\n",
    "    map_text_files(from_txt_files, to_txt_files, mapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked_gen(l, chunk_size):\n",
    "    for i in range(0, len(l), chunk_size):\n",
    "        yield l[i:i + chunk_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_counter_from_files(txt_files):\n",
    "    texts = [open(file).read().split('\\n') for file in txt_files]\n",
    "    lines = []\n",
    "    for txt_lines in texts:\n",
    "        lines.extend([line.split() for line in txt_lines if line])\n",
    "\n",
    "    return count_chars(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_files = [os.path.join(results_dir,file_path) for file_path in  os.listdir(results_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [26:52<00:00, 30.42s/it]\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 500\n",
    "token_counters = []\n",
    "for files in tqdm.tqdm(chunked_gen(txt_files, chunk_size), total = len(txt_files)//chunk_size + 1):\n",
    "    token_counters.append(get_token_counter_from_files(files))\n",
    "sum_token_counter = sum(token_counters, collections.Counter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = ['<S>', '</S>', '<UNK>'] + [token for token, c in sum_token_counter.most_common()]\n",
    "open(tokens_file, 'wt').write('\\n'.join(tokens)) \n",
    "open(chars_file, 'wt').write('\\n'.join(chars)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
